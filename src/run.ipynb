{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision.utils import save_image\n",
    "from net import MaskGenerator, ResiduePredictor\n",
    "from mydataset import MyDataset\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'sample'\n",
    "num_primary_color = 7 \n",
    "csv_path = 'sample.csv' \n",
    "resize_scale_factor = 1 \n",
    "\n",
    "#img_name = 'image_test_walid_2.jpeg'; manual_color_0 = [98, 12, 15]; manual_color_1 = [138, 206, 225]; manual_color_2 = [226, 179, 159]; manual_color_3 = [69, 173, 198]; manual_color_4 = [213, 215, 221]; manual_color_5 = [85,26,20]; manual_color_6 = [160,217,214]; \n",
    "\n",
    "#img_name = 'palette1.jpeg'; manual_color_0 = [98, 12, 15]; manual_color_1 = [138, 206, 225]; manual_color_2 = [226, 179, 159]; manual_color_3 = [69, 173, 198]; manual_color_4 = [213, 215, 221]; manual_color_5 = [85,26,20]; manual_color_6 = [160,217,214]; \n",
    "\n",
    "#img_name = 'palette2.jpeg'; manual_color_0 = [98, 12, 15]; manual_color_1 = [138, 206, 225]; manual_color_2 = [226, 179, 159]; manual_color_3 = [69, 173, 198]; manual_color_4 = [213, 215, 221]; manual_color_5 = [85,26,20]; manual_color_6 = [160,217,214]; \n",
    "\n",
    "img_name = 'palette3.jpeg'; manual_color_0 = [98, 12, 15]; manual_color_1 = [138, 206, 225]; manual_color_2 = [226, 179, 159]; manual_color_3 = [69, 173, 198]; manual_color_4 = [213, 215, 221]; manual_color_5 = [85,26,20]; manual_color_6 = [160,217,214]; \n",
    "\n",
    "\n",
    "img_path = '../dataset/test/' + img_name\n",
    "\n",
    "path_mask_generator = 'results/' + run_name + '/mask_generator.pth'\n",
    "path_residue_predictor = 'results/' + run_name + '/residue_predictor.pth'\n",
    "\n",
    "if num_primary_color == 7:\n",
    "    manual_colors = np.array([manual_color_0, manual_color_1, manual_color_2, manual_color_3,\\\n",
    "                                               manual_color_4, manual_color_5, manual_color_6]) /255\n",
    "elif num_primary_color == 6:\n",
    "    manual_colors = np.array([manual_color_0, manual_color_1, manual_color_2, manual_color_3,\\\n",
    "                                               manual_color_4, manual_color_5]) /255\n",
    "elif num_primary_color == 4:\n",
    "    manual_colors = np.array([manual_color_0, manual_color_1, manual_color_2, manual_color_3])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('results/%s/%s' % (run_name, img_name))\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MaskGenerator(\n",
       "  (conv1): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (conv2): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (conv3): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (deconv1): ConvTranspose2d(192, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "  (deconv2): ConvTranspose2d(192, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "  (deconv3): ConvTranspose2d(96, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "  (conv4): Conv2d(51, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(24, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bnde1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bnde2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bnde3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "test_dataset = MyDataset(csv_path, num_primary_color, mode='test')\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "    \n",
    "mask_generator = MaskGenerator(num_primary_color).to(device)\n",
    "residue_predictor = ResiduePredictor(num_primary_color).to(device)\n",
    "\n",
    "mask_generator.load_state_dict(torch.load(path_mask_generator, map_location=map_location))\n",
    "residue_predictor.load_state_dict(torch.load(path_residue_predictor, map_location=map_location))\n",
    "\n",
    "mask_generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_color(primary_color_layers, manual_colors):\n",
    "    temp_primary_color_layers = primary_color_layers.clone()\n",
    "    for layer in range(len(manual_colors)):\n",
    "        for color in range(3):\n",
    "                temp_primary_color_layers[:,layer,color,:,:].fill_(manual_colors[layer][color])\n",
    "    return temp_primary_color_layers\n",
    "\n",
    "\n",
    "def cut_edge(target_img):\n",
    "    target_img = F.interpolate(target_img, scale_factor=resize_scale_factor, mode='area')\n",
    "    h = target_img.size(2)\n",
    "    w = target_img.size(3)\n",
    "    h = h - (h % 8)\n",
    "    w = w - (w % 8)\n",
    "    target_img = target_img[:,:,:h,:w]\n",
    "    return target_img\n",
    "\n",
    "def alpha_normalize(alpha_layers):\n",
    "    return alpha_layers / (alpha_layers.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "def read_backimage():\n",
    "    img = cv2.imread('../dataset/backimage.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.transpose((2,0,1))\n",
    "    img = img/255\n",
    "    img = torch.from_numpy(img.astype(np.float32))\n",
    "\n",
    "    return img.view(1,3,256,256).to(device)\n",
    "\n",
    "backimage = read_backimage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guided_filter_pytorch.guided_filter import GuidedFilter\n",
    "def proc_guidedfilter(alpha_layers, guide_img):\n",
    "    guide_img = (guide_img[:, 0, :, :]*0.299 + guide_img[:, 1, :, :]*0.587 + guide_img[:, 2, :, :]*0.114).unsqueeze(1)\n",
    "    for i in range(alpha_layers.size(1)):\n",
    "        layer = alpha_layers[:, i, :, :, :]\n",
    "        processed_layer = GuidedFilter(3, 1*1e-6)(guide_img, layer)\n",
    "        if i == 0: \n",
    "            processed_alpha_layers = processed_layer.unsqueeze(1)\n",
    "        else:\n",
    "            processed_alpha_layers = torch.cat((processed_alpha_layers, processed_layer.unsqueeze(1)), dim=1)\n",
    "    \n",
    "    return processed_alpha_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_number = [0, 1]\n",
    "mask_path = 'path/to/mask.image'\n",
    "\n",
    "def load_mask(mask_path):\n",
    "    mask = cv2.imread(mask_path, 0) \n",
    "    mask[mask<128] = 0.\n",
    "    mask[mask >= 128] = 1.\n",
    "    mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float().cuda()\n",
    "    return mask\n",
    "\n",
    "def mask_operate(alpha_layers, target_layer_number, mask_path):\n",
    "    layer_A = alpha_layers[:, target_layer_number[0], :, :, :]\n",
    "    layer_B = alpha_layers[:, target_layer_number[1], :, :, :]\n",
    "    \n",
    "    layer_AB = layer_A + layer_B\n",
    "    mask = load_mask(mask_path)\n",
    "    mask = cut_edge(mask)\n",
    "    \n",
    "    layer_A = layer_AB * mask\n",
    "    layer_B = layer_AB * (1. - mask)\n",
    "    \n",
    "    return_alpha_layers = alpha_layers.clone()\n",
    "    return_alpha_layers[:, target_layer_number[0], :, :, :] = layer_A\n",
    "    return_alpha_layers[:, target_layer_number[1], :, :, :] = layer_B\n",
    "    \n",
    "    return return_alpha_layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.imgs_path[0] = img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "img # 0\n",
      "0.059426307678222656\n",
      "Saved to results/sample/palette3.jpeg/...\n"
     ]
    }
   ],
   "source": [
    "img_number = 0\n",
    "mean_estimation_time = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (target_img, primary_color_layers) in enumerate(test_loader):\n",
    "        if batch_idx != img_number:\n",
    "            print('Skip ', batch_idx)\n",
    "            continue\n",
    "        print('img #', batch_idx)\n",
    "        target_img = cut_edge(target_img)\n",
    "        target_img = target_img.to(device) \n",
    "        primary_color_layers = primary_color_layers.to(device)\n",
    "        primary_color_layers = replace_color(primary_color_layers, manual_colors)\n",
    "        start_time = time.time()\n",
    "        primary_color_pack = primary_color_layers.view(primary_color_layers.size(0), -1 , primary_color_layers.size(3), primary_color_layers.size(4))\n",
    "        primary_color_pack = cut_edge(primary_color_pack)\n",
    "        primary_color_layers = primary_color_pack.view(primary_color_pack.size(0),-1,3,primary_color_pack.size(2), primary_color_pack.size(3))\n",
    "        pred_alpha_layers_pack = mask_generator(target_img, primary_color_pack)\n",
    "        pred_alpha_layers = pred_alpha_layers_pack.view(target_img.size(0), -1, 1, target_img.size(2), target_img.size(3))\n",
    "        \n",
    "        processed_alpha_layers = alpha_normalize(pred_alpha_layers) \n",
    "        processed_alpha_layers = proc_guidedfilter(processed_alpha_layers, target_img) \n",
    "        processed_alpha_layers = alpha_normalize(processed_alpha_layers)  \n",
    "        \n",
    "        mono_color_layers = torch.cat((primary_color_layers, processed_alpha_layers), 2) #shape: bn, ln, 4, h, w\n",
    "        mono_color_layers_pack = mono_color_layers.view(target_img.size(0), -1 , target_img.size(2), target_img.size(3))\n",
    "        residue_pack  = residue_predictor(target_img, mono_color_layers_pack)\n",
    "        residue = residue_pack.view(target_img.size(0), -1, 3, target_img.size(2), target_img.size(3))\n",
    "        pred_unmixed_rgb_layers = torch.clamp((primary_color_layers + residue), min=0., max=1.0)\n",
    "        reconst_img = (pred_unmixed_rgb_layers * processed_alpha_layers).sum(dim=1)\n",
    "        end_time = time.time()\n",
    "        estimation_time = end_time - start_time\n",
    "        print(estimation_time)\n",
    "        mean_estimation_time += estimation_time\n",
    "        \n",
    "        if True:\n",
    "            save_layer_number = 0\n",
    "            save_image(primary_color_layers[save_layer_number,:,:,:,:],\n",
    "                   'results/%s/%s/test' % (run_name, img_name) + '_img-%02d_primary_color_layers.png' % batch_idx)\n",
    "            save_image(reconst_img[save_layer_number,:,:,:].unsqueeze(0),\n",
    "                   'results/%s/%s/test' % (run_name, img_name)  + '_img-%02d_reconst_img.png' % batch_idx)\n",
    "            save_image(target_img[save_layer_number,:,:,:].unsqueeze(0),\n",
    "                   'results/%s/%s/test' % (run_name, img_name)  + '_img-%02d_target_img.png' % batch_idx)\n",
    "\n",
    "            RGBA_layers = torch.cat((pred_unmixed_rgb_layers, processed_alpha_layers), dim=2) \n",
    "            RGBA_layers = RGBA_layers[0]\n",
    "            for i in range(len(RGBA_layers)):\n",
    "                save_image(RGBA_layers[i, :, :, :], 'results/%s/%s/img-%02d_layer-%02d.png' % (run_name, img_name, batch_idx, i) )\n",
    "            print('Saved to results/%s/%s/...' % (run_name, img_name))\n",
    "            \n",
    "        if False:\n",
    "            mono_RGBA_layers = torch.cat((primary_color_layers, processed_alpha_layers), dim=2) \n",
    "            mono_RGBA_layers = mono_RGBA_layers[0] \n",
    "            for i in range(len(mono_RGBA_layers)):\n",
    "                save_image(mono_RGBA_layers[i, :, :, :], 'results/%s/%s/mono_img-%02d_layer-%02d.png' % (run_name, img_name, batch_idx, i) )\n",
    "\n",
    "            save_image((primary_color_layers * processed_alpha_layers).sum(dim=1)[save_layer_number,:,:,:].unsqueeze(0),\n",
    "                   'results/%s/%s/test' % (run_name, img_name)  + '_mono_img-%02d_reconst_img.png' % batch_idx)   \n",
    "        \n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 37.66605876  37.08373478  38.28576344 105.66786723  74.02425252\n  62.12334395 210.88639369 193.62756526 171.38110398   4.61467189\n   3.83550286   5.81568787 214.91317886 163.61534939 104.29410508\n 147.61077284 130.65422581 125.53918248 192.19831804  98.48629969\n  61.04275229]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "num_clusters = 7\n",
    "img_name = 'palette3.jpeg'\n",
    "img_path = '../dataset/test/' + img_name\n",
    "\n",
    "img = cv2.imread(img_path)[:, :, [2, 1, 0]]\n",
    "size = img.shape[:2]\n",
    "vec_img = img.reshape(-1, 3)\n",
    "model = KMeans(n_clusters=num_clusters, n_jobs=-1)\n",
    "pred = model.fit_predict(vec_img)\n",
    "pred_img = np.tile(pred.reshape(*size,1), (1,1,3))\n",
    "\n",
    "center = model.cluster_centers_.reshape(-1)\n",
    "print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "img_name = 'palette3.jpeg'; manual_color_0 = [37, 37, 38]; manual_color_1 = [105, 74, 62]; manual_color_2 = [210, 193, 171]; manual_color_3 = [4, 3, 5]; manual_color_4 = [214, 163, 104]; manual_color_5 = [147, 130, 125]; manual_color_6 = [192, 98, 61]; "
     ]
    }
   ],
   "source": [
    "print('img_name = \\'%s\\';' % img_name, end=\" \")\n",
    "for k, i in enumerate(model.cluster_centers_):\n",
    "    print('manual_color_%d = [' % k + str(i[0].astype('int')) +', '+ str(i[1].astype('int'))+  ', '+ str(i[2].astype('int')) + '];', end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}