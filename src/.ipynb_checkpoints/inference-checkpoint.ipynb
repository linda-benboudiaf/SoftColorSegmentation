{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision.utils import save_image\n",
    "from net import MaskGenerator, ResiduePredictor\n",
    "from mydataset import MyDataset\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User inputs\n",
    "\n",
    "run_name = 'sample'\n",
    "num_primary_color = 7 \n",
    "csv_path = 'sample.csv' \n",
    "resize_scale_factor = 1  \n",
    "\n",
    "# image name and palette color values\n",
    "#img_name = 'apple.jpg'; manual_color_0 = [253, 253, 254]; manual_color_1 = [203, 194, 170]; manual_color_2 = [83, 17, 22]; manual_color_3 = [205, 118, 4]; manual_color_4 = [220, 222, 11]; manual_color_5 = [155, 24, 10]; manual_color_6 = [171, 75, 67]; \n",
    "\n",
    "img_name = 'image_test_walid_2.jpeg'; manual_color_0 = [98, 12, 15]; manual_color_1 = [138, 206, 225]; manual_color_2 = [226, 179, 159]; manual_color_3 = [69, 173, 198]; manual_color_4 = [213, 215, 221]; manual_color_5 = [85,26,20]; manual_color_6 = [160,217,214]; \n",
    "\n",
    "#img_name = 'image_test_walid_2.jpeg'; manual_color_0 = [98, 12, 15]; manual_color_1 = [138, 206, 225]; manual_color_2 = [226, 179, 159]; manual_color_3 = [69, 173, 198]; \n",
    "\n",
    "#img_name = 'buildings.png'; manual_color_0 = [59, 66, 80]; manual_color_1 = [12, 12, 11]; manual_color_2 = [65, 56, 43]; manual_color_3 = [78, 92, 120]; manual_color_4 = [223, 192, 124]; manual_color_5 = [128, 102, 63]; manual_color_6 = [36, 36, 33]; \n",
    "#img_name = 'castle.jpg'; manual_color_0 = [60, 81, 116]; manual_color_1 = [175, 198, 215]; manual_color_2 = [0, 0, 0]; manual_color_3 = [114, 149, 185]; manual_color_4 = [142, 172, 198]; manual_color_5 = [92, 116, 149]; manual_color_6 = [226, 221, 222]; \n",
    "#img_name = 'girls.png'; manual_color_0 = [125, 116, 105]; manual_color_1 = [155, 162, 191]; manual_color_2 = [52, 60, 39]; manual_color_3 = [87, 120, 196]; manual_color_4 = [87, 107, 56]; manual_color_5 = [19, 26, 10]; manual_color_6 = [183,187,209]; \n",
    "#img_name = 'rowboat1.png'; manual_color_2 = [175, 77, 13]; manual_color_1 = [51, 45, 39]; manual_color_0 = [93, 89, 90]; manual_color_6 = [245, 141, 84]; manual_color_4 = [14, 13, 7]; manual_color_5 = [62, 71, 74]; manual_color_3 = [158,153,157]; \n",
    "#img_name = 'scrooge.png'; manual_color_0 = [254, 254, 254]; manual_color_1 = [78, 71, 65]; manual_color_2 = [211, 182, 135]; manual_color_3 = [165, 127, 100]; manual_color_4 = [40, 38, 34]; manual_color_5 = [112, 45, 31]; manual_color_6 = [177, 57, 35]; \n",
    "#img_name = 'turquoise.png'; manual_color_0 = [86, 59, 67]; manual_color_1 = [121, 132, 148]; manual_color_2 = [228, 186, 156]; manual_color_3 = [53, 35, 34]; manual_color_4 = [190, 135, 122]; manual_color_5 = [94, 152, 154]; manual_color_6 = [254,229,216]; \n",
    "#img_name = 'orange.png'; manual_color_0 = [79, 81, 59]; manual_color_1 = [112, 117, 105]; manual_color_2 = [137, 92, 41]; manual_color_3 = [201,214,197]; manual_color_4 = [42, 53, 49]; manual_color_5 = [168, 130, 40]; manual_color_6 = [114, 60, 31]; \n",
    "\n",
    "####\n",
    "\n",
    "img_path = '../dataset/test/' + img_name\n",
    "\n",
    "path_mask_generator = 'results/' + run_name + '/mask_generator.pth'\n",
    "path_residue_predictor = 'results/' + run_name + '/residue_predictor.pth'\n",
    "\n",
    "if num_primary_color == 7:\n",
    "    manual_colors = np.array([manual_color_0, manual_color_1, manual_color_2, manual_color_3,\\\n",
    "                                               manual_color_4, manual_color_5, manual_color_6]) /255\n",
    "elif num_primary_color == 6:\n",
    "    manual_colors = np.array([manual_color_0, manual_color_1, manual_color_2, manual_color_3,\\\n",
    "                                               manual_color_4, manual_color_5]) /255\n",
    "elif num_primary_color == 4:\n",
    "    manual_colors = np.array([manual_color_0, manual_color_1, manual_color_2, manual_color_3])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('results/%s/%s' % (run_name, img_name))\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MaskGenerator:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([48, 24, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 15, 3, 3]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([96, 48, 3, 3]) from checkpoint, the shape in current model is torch.Size([60, 30, 3, 3]).\n\tsize mismatch for conv3.weight: copying a param with shape torch.Size([192, 96, 3, 3]) from checkpoint, the shape in current model is torch.Size([120, 60, 3, 3]).\n\tsize mismatch for deconv1.weight: copying a param with shape torch.Size([192, 96, 3, 3]) from checkpoint, the shape in current model is torch.Size([120, 60, 3, 3]).\n\tsize mismatch for deconv2.weight: copying a param with shape torch.Size([192, 48, 3, 3]) from checkpoint, the shape in current model is torch.Size([120, 30, 3, 3]).\n\tsize mismatch for deconv3.weight: copying a param with shape torch.Size([96, 48, 3, 3]) from checkpoint, the shape in current model is torch.Size([60, 30, 3, 3]).\n\tsize mismatch for conv4.weight: copying a param with shape torch.Size([24, 51, 3, 3]) from checkpoint, the shape in current model is torch.Size([15, 33, 3, 3]).\n\tsize mismatch for conv4.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for conv5.weight: copying a param with shape torch.Size([7, 24, 3, 3]) from checkpoint, the shape in current model is torch.Size([4, 15, 3, 3]).\n\tsize mismatch for conv5.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn2.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn2.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn2.running_mean: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn2.running_var: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn3.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bn3.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bn3.running_mean: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bn3.running_var: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bnde1.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde1.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde1.running_mean: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde1.running_var: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde2.weight: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde2.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde2.running_mean: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde2.running_var: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.weight: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.running_mean: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.running_var: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn4.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for bn4.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for bn4.running_mean: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for bn4.running_var: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-84c86694b6ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# load params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmask_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_mask_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mresidue_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_residue_predictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MaskGenerator:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([48, 24, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 15, 3, 3]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([96, 48, 3, 3]) from checkpoint, the shape in current model is torch.Size([60, 30, 3, 3]).\n\tsize mismatch for conv3.weight: copying a param with shape torch.Size([192, 96, 3, 3]) from checkpoint, the shape in current model is torch.Size([120, 60, 3, 3]).\n\tsize mismatch for deconv1.weight: copying a param with shape torch.Size([192, 96, 3, 3]) from checkpoint, the shape in current model is torch.Size([120, 60, 3, 3]).\n\tsize mismatch for deconv2.weight: copying a param with shape torch.Size([192, 48, 3, 3]) from checkpoint, the shape in current model is torch.Size([120, 30, 3, 3]).\n\tsize mismatch for deconv3.weight: copying a param with shape torch.Size([96, 48, 3, 3]) from checkpoint, the shape in current model is torch.Size([60, 30, 3, 3]).\n\tsize mismatch for conv4.weight: copying a param with shape torch.Size([24, 51, 3, 3]) from checkpoint, the shape in current model is torch.Size([15, 33, 3, 3]).\n\tsize mismatch for conv4.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for conv5.weight: copying a param with shape torch.Size([7, 24, 3, 3]) from checkpoint, the shape in current model is torch.Size([4, 15, 3, 3]).\n\tsize mismatch for conv5.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn2.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn2.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn2.running_mean: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn2.running_var: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bn3.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bn3.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bn3.running_mean: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bn3.running_var: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([120]).\n\tsize mismatch for bnde1.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde1.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde1.running_mean: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde1.running_var: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for bnde2.weight: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde2.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde2.running_mean: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde2.running_var: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.weight: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.running_mean: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bnde3.running_var: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for bn4.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for bn4.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for bn4.running_mean: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15]).\n\tsize mismatch for bn4.running_var: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([15])."
     ]
    }
   ],
   "source": [
    "test_dataset = MyDataset(csv_path, num_primary_color, mode='test')\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    )\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# define model\n",
    "mask_generator = MaskGenerator(num_primary_color).to(device)\n",
    "residue_predictor = ResiduePredictor(num_primary_color).to(device)\n",
    "\n",
    "# load params\n",
    "mask_generator.load_state_dict(torch.load(path_mask_generator))\n",
    "residue_predictor.load_state_dict(torch.load(path_residue_predictor))\n",
    "\n",
    "# eval mode\n",
    "mask_generator.eval()\n",
    "residue_predictor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_color(primary_color_layers, manual_colors):\n",
    "    temp_primary_color_layers = primary_color_layers.clone()\n",
    "    for layer in range(len(manual_colors)):\n",
    "        for color in range(3):\n",
    "                temp_primary_color_layers[:,layer,color,:,:].fill_(manual_colors[layer][color])\n",
    "    return temp_primary_color_layers\n",
    "\n",
    "\n",
    "def cut_edge(target_img):\n",
    "    target_img = F.interpolate(target_img, scale_factor=resize_scale_factor, mode='area')\n",
    "    h = target_img.size(2)\n",
    "    w = target_img.size(3)\n",
    "    h = h - (h % 8)\n",
    "    w = w - (w % 8)\n",
    "    target_img = target_img[:,:,:h,:w]\n",
    "    return target_img\n",
    "\n",
    "def alpha_normalize(alpha_layers):\n",
    "    return alpha_layers / (alpha_layers.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "def read_backimage():\n",
    "    img = cv2.imread('../dataset/backimage.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.transpose((2,0,1))\n",
    "    img = img/255\n",
    "    img = torch.from_numpy(img.astype(np.float32))\n",
    "\n",
    "    return img.view(1,3,256,256).to(device)\n",
    "\n",
    "backimage = read_backimage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guided_filter_pytorch.guided_filter import GuidedFilter\n",
    "\n",
    "def proc_guidedfilter(alpha_layers, guide_img):\n",
    "    # target_img． bn, 3, h, w\n",
    "    guide_img = (guide_img[:, 0, :, :]*0.299 + guide_img[:, 1, :, :]*0.587 + guide_img[:, 2, :, :]*0.114).unsqueeze(1)\n",
    "        \n",
    "    for i in range(alpha_layers.size(1)):\n",
    "        # layer bn, 1, h, w\n",
    "        layer = alpha_layers[:, i, :, :, :]\n",
    "        \n",
    "        processed_layer = GuidedFilter(3, 1*1e-6)(guide_img, layer)\n",
    "        if i == 0: \n",
    "            processed_alpha_layers = processed_layer.unsqueeze(1)\n",
    "        else:\n",
    "            processed_alpha_layers = torch.cat((processed_alpha_layers, processed_layer.unsqueeze(1)), dim=1)\n",
    "    \n",
    "    return processed_alpha_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_number = [0, 1]\n",
    "mask_path = 'path/to/mask.image'\n",
    "\n",
    "## Define functions for mask operation.\n",
    "def load_mask(mask_path):\n",
    "    mask = cv2.imread(mask_path, 0) \n",
    "    mask[mask<128] = 0.\n",
    "    mask[mask >= 128] = 1.\n",
    "    # tensorに変換する\n",
    "    mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float().cuda()\n",
    "    \n",
    "    return mask\n",
    "        \n",
    "\n",
    "def mask_operate(alpha_layers, target_layer_number, mask_path):\n",
    "    layer_A = alpha_layers[:, target_layer_number[0], :, :, :]\n",
    "    layer_B = alpha_layers[:, target_layer_number[1], :, :, :]\n",
    "    \n",
    "    layer_AB = layer_A + layer_B\n",
    "    mask = load_mask(mask_path)\n",
    "    \n",
    "    mask = cut_edge(mask)\n",
    "    \n",
    "    layer_A = layer_AB * mask\n",
    "    layer_B = layer_AB * (1. - mask)\n",
    "    \n",
    "    return_alpha_layers = alpha_layers.clone()\n",
    "    return_alpha_layers[:, target_layer_number[0], :, :, :] = layer_A\n",
    "    return_alpha_layers[:, target_layer_number[1], :, :, :] = layer_B\n",
    "    \n",
    "    return return_alpha_layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.imgs_path[0] = img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "img # 0\n",
      "0.011971235275268555\n",
      "Saved to results/sample/image_test_walid_2.jpeg/...\n"
     ]
    }
   ],
   "source": [
    "print('Start!')\n",
    "img_number = 0\n",
    "\n",
    "\n",
    "mean_estimation_time = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (target_img, primary_color_layers) in enumerate(test_loader):\n",
    "        if batch_idx != img_number:\n",
    "            print('Skip ', batch_idx)\n",
    "            continue\n",
    "        print('img #', batch_idx)\n",
    "        target_img = cut_edge(target_img)\n",
    "        target_img = target_img.to(device) # bn, 3ch, h, w\n",
    "        primary_color_layers = primary_color_layers.to(device)\n",
    "        ##\n",
    "        primary_color_layers = replace_color(primary_color_layers, manual_colors)\n",
    "        ##\n",
    "        start_time = time.time()\n",
    "        primary_color_pack = primary_color_layers.view(primary_color_layers.size(0), -1 , primary_color_layers.size(3), primary_color_layers.size(4))\n",
    "        primary_color_pack = cut_edge(primary_color_pack)\n",
    "        primary_color_layers = primary_color_pack.view(primary_color_pack.size(0),-1,3,primary_color_pack.size(2), primary_color_pack.size(3))\n",
    "        pred_alpha_layers_pack = mask_generator(target_img, primary_color_pack)\n",
    "        pred_alpha_layers = pred_alpha_layers_pack.view(target_img.size(0), -1, 1, target_img.size(2), target_img.size(3))\n",
    "        \n",
    "        ## Alpha Layer Proccessing\n",
    "        processed_alpha_layers = alpha_normalize(pred_alpha_layers) \n",
    "        processed_alpha_layers = proc_guidedfilter(processed_alpha_layers, target_img) # Option\n",
    "        processed_alpha_layers = alpha_normalize(processed_alpha_layers)  # Option\n",
    "        \n",
    "        ##\n",
    "        mono_color_layers = torch.cat((primary_color_layers, processed_alpha_layers), 2) #shape: bn, ln, 4, h, w\n",
    "        mono_color_layers_pack = mono_color_layers.view(target_img.size(0), -1 , target_img.size(2), target_img.size(3))\n",
    "        residue_pack  = residue_predictor(target_img, mono_color_layers_pack)\n",
    "        residue = residue_pack.view(target_img.size(0), -1, 3, target_img.size(2), target_img.size(3))\n",
    "        pred_unmixed_rgb_layers = torch.clamp((primary_color_layers + residue), min=0., max=1.0)\n",
    "        reconst_img = (pred_unmixed_rgb_layers * processed_alpha_layers).sum(dim=1)\n",
    "        end_time = time.time()\n",
    "        estimation_time = end_time - start_time\n",
    "        print(estimation_time)\n",
    "        mean_estimation_time += estimation_time\n",
    "        \n",
    "        if True:\n",
    "            save_layer_number = 0\n",
    "            save_image(primary_color_layers[save_layer_number,:,:,:,:],\n",
    "                   'results/%s/%s/test' % (run_name, img_name) + '_img-%02d_primary_color_layers.png' % batch_idx)\n",
    "            save_image(reconst_img[save_layer_number,:,:,:].unsqueeze(0),\n",
    "                   'results/%s/%s/test' % (run_name, img_name)  + '_img-%02d_reconst_img.png' % batch_idx)\n",
    "            save_image(target_img[save_layer_number,:,:,:].unsqueeze(0),\n",
    "                   'results/%s/%s/test' % (run_name, img_name)  + '_img-%02d_target_img.png' % batch_idx)\n",
    "\n",
    "            RGBA_layers = torch.cat((pred_unmixed_rgb_layers, processed_alpha_layers), dim=2) # out: bn, ln, 4, h, w\n",
    "            RGBA_layers = RGBA_layers[0] # ln, 4. h, w\n",
    "            for i in range(len(RGBA_layers)):\n",
    "                save_image(RGBA_layers[i, :, :, :], 'results/%s/%s/img-%02d_layer-%02d.png' % (run_name, img_name, batch_idx, i) )\n",
    "            print('Saved to results/%s/%s/...' % (run_name, img_name))\n",
    "            \n",
    "        if False:\n",
    "            mono_RGBA_layers = torch.cat((primary_color_layers, processed_alpha_layers), dim=2) # out: bn, ln, 4, h, w\n",
    "            mono_RGBA_layers = mono_RGBA_layers[0] # ln, 4. h, w\n",
    "            for i in range(len(mono_RGBA_layers)):\n",
    "                save_image(mono_RGBA_layers[i, :, :, :], 'results/%s/%s/mono_img-%02d_layer-%02d.png' % (run_name, img_name, batch_idx, i) )\n",
    "\n",
    "            save_image((primary_color_layers * processed_alpha_layers).sum(dim=1)[save_layer_number,:,:,:].unsqueeze(0),\n",
    "                   'results/%s/%s/test' % (run_name, img_name)  + '_mono_img-%02d_reconst_img.png' % batch_idx)   \n",
    "        \n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Appendix) Save alpha channel and RGB channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理まえのアルファを保存\n",
    "for i in range(len(pred_alpha_layers[0])):\n",
    "            save_image(pred_alpha_layers[0,i, :, :, :], 'results/%s/%s/pred-alpha-00_layer-%02d.png' % (run_name, img_name, i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理後のアルファの保存 processed_alpha_layers\n",
    "for i in range(len(processed_alpha_layers[0])):\n",
    "            save_image(processed_alpha_layers[0,i, :, :, :], 'results/%s/%s/proc-alpha-00_layer-%02d.png' % (run_name, img_name, i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理後のRGBの保存\n",
    "for i in range(len(pred_unmixed_rgb_layers[0])):\n",
    "    save_image(pred_unmixed_rgb_layers[0,i, :, :, :], 'results/%s/%s/rgb-00_layer-%02d.png' % (run_name, img_name, i) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Appendix) K-means for culculating pallete colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linda/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182.07520403 200.75346442 204.42810977 114.11328523 190.86902644\n",
      " 209.39607859 149.38478185   8.29884817  23.57186736  70.0694128\n",
      "  83.25836985  87.33970202]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "### User inputs\n",
    "num_clusters = 7\n",
    "img_name = 'image_test_walid_2.jpeg'\n",
    "img_path = '../dataset/test/' + img_name\n",
    "\n",
    "img = cv2.imread(img_path)[:, :, [2, 1, 0]]\n",
    "size = img.shape[:2]\n",
    "vec_img = img.reshape(-1, 3)\n",
    "model = KMeans(n_clusters=num_clusters, n_jobs=-1)\n",
    "pred = model.fit_predict(vec_img)\n",
    "pred_img = np.tile(pred.reshape(*size,1), (1,1,3))\n",
    "\n",
    "center = model.cluster_centers_.reshape(-1)\n",
    "print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_name = 'image_test_walid_2.jpeg'; manual_color_0 = [182, 200, 204]; manual_color_1 = [114, 190, 209]; manual_color_2 = [149, 8, 23]; manual_color_3 = [70, 83, 87]; "
     ]
    }
   ],
   "source": [
    "# Reshape for an input\n",
    "print('img_name = \\'%s\\';' % img_name, end=\" \")\n",
    "for k, i in enumerate(model.cluster_centers_):\n",
    "    print('manual_color_%d = [' % k + str(i[0].astype('int')) +', '+ str(i[1].astype('int'))+  ', '+ str(i[2].astype('int')) + '];', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
